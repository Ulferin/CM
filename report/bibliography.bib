@book{MLmitchell,
    author = {Mitchell, Thomas M.},
    title = {Machine Learning},
    year = {1997},
    % isbn = {0070428077},
    publisher = {McGraw-Hill, Inc.},
    % address = {USA},
    edition = {1},
    abstract = {This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical and coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text for college students. Table of contents Chapter 1. Introduction Chapter 2. Concept Learning and the General-to-Specific Ordering Chapter 3. Decision Tree Learning Chapter 4. Artificial Neural Networks Chapter 5. Evaluating Hypotheses Chapter 6. Bayesian Learning Chapter 7. Computational Learning Theory Chapter 8. Instance-Based Learning Chapter 9. Inductive Logic Programming Chapter 10. Analytical Learning Chapter 11. Combining Inductive and Analytical Learning Chapter 12. Reinforcement Learning.}
}

@book{haykin_neural_2009,
	edition = {3rd ed},
	title = {Neural networks and learning machines},
% 	isbn = {9780131471399},
	publisher = {Prentice Hall},
	author = {Haykin, Simon S.},
	year = {2009},
	keywords = {Neural networks (Computer science), Adaptive filters},
}

@book{bengio,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
title = {Deep Learning},
year = {2016},
% isbn = {0262035618},
publisher = {The MIT Press},
abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.}
}

@book{Bau,
%   added-at = {2010-09-19T02:35:23.000+0200},
  author = {Trefethen, Lloyd N. and Bau, David},
%   biburl = {https://www.bibsonomy.org/bibtex/2e45a2ed5ccc6dc12721cde613217c222/ytyoun},
%   interhash = {1e7e7a44cbff3092be50a71fe056c8ec},
%   intrahash = {e45a2ed5ccc6dc12721cde613217c222},
%   isbn = {0898713617},
%   keywords = {characteristic eigenvalues linear.algebra matrix numerical numerical.analysis polynomial secular.equation textbook},
  publisher = {SIAM},
%   timestamp = {2017-11-25T07:18:16.000+0100},
  title = {Numerical Linear Algebra},
  year = 1997
}

@book{elden,
%   added-at = {2019-07-09T00:00:00.000+0200},
  author = {Elden, Lars},
%   biburl = {https://www.bibsonomy.org/bibtex/2368720c2d3a8c984334272e82cb31463/dblp},
%   interhash = {b0322a01c5c48915a9d443ba0528e4b4},
%   intrahash = {368720c2d3a8c984334272e82cb31463},
%   isbn = {978-0-89871-626-9},
%   keywords = {dblp},
%   pages = {I-X, 1-224},
  publisher = {SIAM},
%   series = {Fundamentals of algorithms},
%   timestamp = {2019-07-10T11:37:46.000+0200},
  title = {Matrix methods in data mining and pattern recognition.},
  volume = 4,
  year = 2007
}

@InProceedings{momentum,
  title = 	 {On the importance of initialization and momentum in deep learning},
  author = 	 {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1139--1147},
  year = 	 {2013},
%   editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
%   number =       {3},
%   series = 	 {Proceedings of Machine Learning Research},
%   address = 	 {Atlanta, Georgia, USA},
%   month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/sutskever13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/sutskever13.html},
%   abstract = 	 {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }
}

@book {nesterov,
    title = {A method of solving a convex programming problem with convergence
rate O(1/$k^2$)},
    author = {Nesterov, Y.},
    year = {1983},
    publisher = {Soviet Mathematics Doklady}
}

@book{nocedal,
% 	address = {New York},
	edition = {2nd ed},
	series = {Springer series in operations research},
	title = {Numerical optimization},
% 	isbn = {9780387303031},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
% 	note = {OCLC: ocm68629100},
% 	keywords = {Mathematical optimization},
}

@book{boyd,
    place={Cambridge},
    title={Convex Optimization},
    % DOI={10.1017/CBO9780511804441},
    publisher={Cambridge University Press},
    author={Boyd, Stephen and Vandenberghe, Lieven},
    year={2004}
}

@book{ml,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
title = {Understanding Machine Learning: From Theory to Algorithms},
year = {2014},
% isbn = {1107057132},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.}
}

@article{subgrad_boyd,
author = {Boyd, Stephen and Mutapcic, Almir},
year = {2003},
% month = {01},
pages = {},
title = {Subgradient Methods},
volume = {2004},
journal = {lecture notes of EE392o, Stanford University, Autumn Quarter}
}

@article{subgrad_fra,
author = {Frangioni, Antonio and Gendron, Bernard and Gorgone, Enrico},
year = {2017},
title = {On the computational efficiency of subgradient methods: a case study with Lagrangian bounds},
volume = {9},
journal = {Mathematical Programming Computation}
}

@book{shetty,
% 	address = {Hoboken, N.J},
	edition = {3rd ed},
	title = {Nonlinear programming: theory and algorithms},
% 	isbn = {9780471486008},
	shorttitle = {Nonlinear programming},
	publisher = {Wiley-Interscience},
	author = {Bazaraa, M. S. and Sherali, Hanif D. and Shetty, C. M.},
	year = {2006},
% 	note = {OCLC: ocm61478842},
	keywords = {Nonlinear programming},
}

@online{notes_subgrad,
author = {Sun, Yuekai},
year = {2015},
url = {http://web.stanford.edu/class/msande318/notes/notes-first-order-nonsmooth.pdf},
title = {Notes on first-order methods for minimizing non-smooth functions.}
}

@misc{bottou2018opt,
      title={Optimization Methods for Large-Scale Machine Learning}, 
      author={LÃ©on Bottou and Frank E. Curtis and Jorge Nocedal},
      year={2018},
      eprint={1606.04838},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sgdunified,
      title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization}, 
      author={Tianbao Yang and Qihang Lin and Zhe Li},
      year={2016},
      eprint={1604.03257},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@book{nonlinearrus,
author = {Ruszczynski, Andrzej},
title = {Nonlinear Optimization},
year = {2006},
isbn = {0691119155},
publisher = {Princeton University Press},
address = {USA},
abstract = {Optimization is one of the most important areas of modern applied mathematics, with applications in fields from engineering and economics to finance, statistics, management science, and medicine. While many books have addressed its various aspects, Nonlinear Optimization is the first comprehensive treatment that will allow graduate students and researchers to understand its modern ideas, principles, and methods within a reasonable time, but without sacrificing mathematical precision. Andrzej Ruszczynski, a leading expert in the optimization of nonlinear stochastic systems, integrates the theory and the methods of nonlinear optimization in a unified, clear, and mathematically rigorous fashion, with detailed and easy-to-follow proofs illustrated by numerous examples and figures. The book covers convex analysis, the theory of optimality conditions, duality theory, and numerical methods for solving unconstrained and constrained optimization problems. It addresses not only classical material but also modern topics such as optimality conditions and numerical methods for problems involving nondifferentiable functions, semidefinite programming, metric regularity and stability theory of set-constrained systems, and sensitivity analysis of optimization problems. Based on a decade's worth of notes the author compiled in successfully teaching the subject, this book will help readers to understand the mathematical foundations of the modern theory and methods of nonlinear optimization and to analyze new problems, develop optimality theory for them, and choose or construct numerical solution methods. It is a must for anyone seriously interested in optimization.}
}

@book{bert_conv,
   title =     {Convex Optimization Algorithms},
   author =    {Dimitri P. Bertsekas},
   publisher = {Athena Scientific},
%   isbn =      {1886529280,9781886529281},
   year =      {2015},
%   series =    {},
   edition =   {1},
%   volume =    {},
%   url =       {http://gen.lib.rus.ec/book/index.php?md5=6c79c097251ed9124f78943d9d685f79}
}

@online{monk_link,
url = {https://archive.ics.uci.edu/ml/datasets/MONK's+Problems},
title = {MONK's Problems Data Set}
}
