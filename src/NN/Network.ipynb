{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3343671",
   "metadata": {},
   "source": [
    "This class represents the Network used in the CM project and it is\n",
    "implemented from scratch following the advices taken during the course\n",
    "of ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a530c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d72b69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from functions import relu, relu_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.load_CUP(\"../../data/ML-CUP20-TR.csv\")\n",
    "# Loads the input and output layers shape\n",
    "input_units = X_train.shape[1]\n",
    "output_units = y_train.shape[1]\n",
    "\n",
    "# Builds the training data for the NN\n",
    "training_data = [ (x,y) for x,y in zip(X_train, y_train)]\n",
    "test_data = [ (x,y) for x,y in zip(X_test, y_test)]\n",
    "\n",
    "sizes =[input_units, 2, output_units]\n",
    "\n",
    "rng = default_rng(0)\n",
    "\n",
    "num_layers = len(sizes)-1\n",
    "sizes = sizes\n",
    "\n",
    "biases = [rng.standard_normal((1,y)) for y in sizes[1:]]\n",
    "weights = [rng.standard_normal((y,x)) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518e225",
   "metadata": {},
   "source": [
    "TODO: check all the list comprehensions, maybe we can substitute them with a numpy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This class represents a standard Neural Network, also called Multilayer Perceptron.\n",
    "It allows to build a network for both classification and regression tasks.\n",
    "\"\"\"    \n",
    "\n",
    "\"\"\"Initializes the network based on the given :param sizes:.\n",
    "Builds the weights and biase vectors for each layer of the network.\n",
    "Each layer will be initialized randomly following the normal distribution. \n",
    "\n",
    ":param sizes: Tuple (i, l1, l2, ..., ln, o) containig the number of units\n",
    "for each layer, where the first and last elements represents, respectively,\n",
    "the input layer and the output layer.\n",
    ":param seed: seed for random number generator used for initializing this network\n",
    "weights and biases. Needed for reproducibility.        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def feedforward(invec):\n",
    "    \"\"\"Applies a feedforward pass to the given input :param in:.\n",
    "    The output generated by the output unit of this network is returned.\n",
    "\n",
    "    :param in: network input vector\n",
    "    :return: network output vector (or scalar)\n",
    "    \"\"\"\n",
    "    out = invec\n",
    "    for b, w in zip(biases[:-1], weights[:-1]):\n",
    "        out = relu(np.dot(w, out.T) + b)\n",
    "\n",
    "    # Last layer is linear for regression tasks\n",
    "    return np.dot(weights[-1], out.T) + biases[-1]\n",
    "\n",
    "\n",
    "def backpropagation(x, y):\n",
    "    # NOTE: this is the backpropagation for a single input example!\n",
    "    # It should perform a feedforward step to compute the current estimated error.\n",
    "    # After that, it uses the computed error to backpropagate the error participation\n",
    "    # of each unit. The error participation will then lead to the definition of the delta\n",
    "    # coefficient used to update the weights and biases for each of the units of the network.\n",
    "\n",
    "    global biases\n",
    "    global weights\n",
    "\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "\n",
    "    # Forward computation\n",
    "    out = x\n",
    "    units_out = [out]\n",
    "    nets = []\n",
    "    for b,w in zip(biases, weights):\n",
    "        net = np.dot(w, out.T) + b\n",
    "        # net += b\n",
    "        out = relu(net)\n",
    "        nets.append(net)\n",
    "        units_out.append(out)\n",
    "\n",
    "    # Backward pass - output unit\n",
    "    delta = (units_out[-1] - y) * relu_prime(nets[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, units_out[-2])\n",
    "\n",
    "    # Backward pass - hidden\n",
    "    for k in range(num_layers-1, 1, -1):\n",
    "        print(k, len(nets))\n",
    "        net = nets[k]\n",
    "        delta = np.dot(weights[k+1], delta) * relu_prime(net)\n",
    "        nabla_b[k] = delta\n",
    "        nabla_w[k] = np.dot(delta, units_out[k-1])\n",
    "        \n",
    "    return nabla_b, nabla_w\n",
    "\n",
    "\n",
    "def update_mini_batch(mini_batch, eta):\n",
    "    \"\"\"Updates the network weights and biases by applying the backpropagation algorithm\n",
    "    to the current set of examples contained in the :param mini_batch:. Computes the deltas\n",
    "    used to update weights as an average over the size of the examples set, using the provided\n",
    "    :param eta: as learning rate.\n",
    "\n",
    "    :param mini_batch: Set of examples to use to update the network weights and biases\n",
    "    :param eta: Learning rate\n",
    "    \"\"\"\n",
    "    global biases\n",
    "    global weights\n",
    "\n",
    "    nabla_b = [ np.zeros(b.shape) for b in biases ]\n",
    "    nabla_w = [ np.zeros(w.shape) for w in weights ]\n",
    "\n",
    "    for x, y in mini_batch:\n",
    "        delta_b, delta_w = backpropagation(x,y)\n",
    "        nabla_b = [ nb + db for nb,db in zip(nabla_b, delta_b)]\n",
    "        nabla_w = [ nw + dw for nw,dw in zip(nabla_w, delta_w) ]\n",
    "\n",
    "    # TODO: probabilmente si pu√≤ fare anche usando solo operazioni di numpy?\n",
    "    weights = [w - (eta/len(mini_batch))*nw for w,nw in zip(weights, nabla_w)]\n",
    "    biases = [b - (eta/len(mini_batch))*nb for b,nb in zip(biases, nabla_b)]\n",
    "\n",
    "\n",
    "def SGD(training_data, epochs, batch_size, eta, test_data=None):\n",
    "    \"\"\"Trains the network using mini-batch stochastic gradient descent,\n",
    "    applied to the training examples in :param training_data: for a given\n",
    "    number of epochs and with the specified learning rate. If :param test_data:\n",
    "    is specified, the learning algorithm will print progresses during the\n",
    "    training phase.\n",
    "\n",
    "    :param training_data: training data represented as a numpy ndarray, each row\n",
    "    represents an example, the last element of each row is the expected output.\n",
    "    :param epochs: number of epochs for training.\n",
    "    :param batch_size: number of examples to use at each backward pass.\n",
    "    :param eta: learning rate.\n",
    "    :param test_data: optional parameter, used to estimate the performance of the network\n",
    "    at each phase, defaults to None.\n",
    "    \"\"\"        \n",
    "    if test_data:\n",
    "        n_test = len(test_data)\n",
    "\n",
    "    n = len(training_data)\n",
    "    for e in range(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [\n",
    "            training_data[k:k+batch_size] for k in range(0, n, batch_size)\n",
    "        ]\n",
    "\n",
    "        for mini_batch in mini_batches:\n",
    "            update_mini_batch(mini_batch, eta)\n",
    "\n",
    "        if test_data:\n",
    "            score = evaluate(test_data)\n",
    "            print(f\"Epoch {e} completed. Score: {score}\")\n",
    "        else:\n",
    "            print(f\"Epoch {e} completed.\")\n",
    "\n",
    "\n",
    "def evaluate(test_data):\n",
    "    # TODO: generalizzare questo metodo, trovare un modo per specificare come valutare\n",
    "    #       il risultato. Dividere in base a classification e regression.\n",
    "    \"\"\"Evaluates the performances of the Network in the current state,\n",
    "    propagating the test examples through the network via a complete feedforward\n",
    "    step. It evaluates the performance using the R2 metric in order to be\n",
    "    comparable with sklearn out-of-the-box NN results.\n",
    "\n",
    "    :param test_data: test data to evaluate the NN\n",
    "    :return: The R2 score as defined by sklearn library\n",
    "    \"\"\"        \n",
    "    # print(f\"test_data: {test_data}\")\n",
    "    \n",
    "    preds = [ np.array(feedforward(x)).reshape(y.shape) for x,y in test_data]\n",
    "    truth = [ y for x,y in test_data ]\n",
    "\n",
    "    # print(f\"preds: {np.array(preds[0]).reshape(1)}, truth: {truth[0]}\")\n",
    "\n",
    "    score = r2_score(preds, truth)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.64042265]]\n"
     ]
    }
   ],
   "source": [
    "out = feedforward(X_train[0])\n",
    "print(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 completed. Score: -0.6407866513386689\n",
      "Epoch 1 completed. Score: -0.6407866513386689\n",
      "Epoch 2 completed. Score: -0.6407866513386689\n",
      "Epoch 3 completed. Score: -0.6407866513386689\n",
      "Epoch 4 completed. Score: -0.6407866513386689\n",
      "Epoch 5 completed. Score: -0.6407866513386689\n",
      "Epoch 6 completed. Score: -0.6407866513386689\n",
      "Epoch 7 completed. Score: -0.6407866513386689\n",
      "Epoch 8 completed. Score: -0.6407866513386689\n",
      "Epoch 9 completed. Score: -0.6407866513386689\n"
     ]
    }
   ],
   "source": [
    "SGD(training_data, 10, 10, 1, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python385jvsc74a57bd068f1cb0c0e290e1c8924208e3bd3ede9e4b325fefb489369279530f117c56ed2",
   "display_name": "Python 3.8.5 64-bit ('CM': pipenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}