\subsection{Subgradient Method}
The \textit{subgradient method} (\textit{SM}) is a minimization method used to minimize non-differentiable convex objective functions. It is not a descent method, the value of the function is not decreasing at every step, in fact the direction negative to a subgradient is not necessarily a direction of descent of the function $f(\cdot)$ \cite{nonlinearrus}.

Given $f: \mathbb{R}^n\to\mathbb{R}$ a convex function not necessarily smooth, to minimize $f$ the \textit{SM} constructs a sequence of iterates $\{x_k\}$ by the iterative formula:
\begin{align*}
    \mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k {g}_k
\end{align*}

where $g_k \in \partial f(x_k)$ is any subgradient of $f(\cdot)$ at $x_k$, $\alpha_k > 0$ is the \textit{k}th stepsize and $x_k$ is the iterate at the step $k$% , as seen in \parencite[Chap. 1]{subgrad_boyd} and \cite{notes_subgrad}
. It is worthwile to recall that a subgradient of $f$ at $x$ is any vector $g$ that satisfies the inequality ${f(y)\geq f(x) + g^T (y-x)}$ for all $y$.

We show a pseudocode for the \textit{SM}:
\begin{algorithm}[H]
    \caption{Basic subgradient method. Assuming starting point $x_1$ and subgradient at each point are given.}
	\label{alg:subalg}
	\begin{algorithmic}[1]
	    \State Initialize $\mathit{x_1}$
	    \State Starting upper bound: $\mathit{UB}_1 \leftarrow f(x_1)$
	    \State Starting optimal point: $x^* \leftarrow x_1$
	    \State $k \leftarrow 1$
		\While{$termination\ conditions\ not\ met$}
		\State Find a subgradient of $f$ in $x_k$: $g_k \in \partial f(x_k)$
		\If {$g_k = 0$}
		\State Terminate with $x^* = x_k$
		\EndIf
		\State Select a direction: $d_k \leftarrow -g_k/\norm{g_k}$
		\State Select a step size: $\alpha_k > 0$
		\State $x_{k+1} \leftarrow x_k + \alpha_k d_k$
        \If{$f(x_{k+1}) < \mathit{UB}_k$}
            \State $\mathit{UB}_{k+1} \leftarrow f(x_{k+1})$
            \State {$x^* \leftarrow x_{k+1}$}
        \Else
            \State $\mathit{UB}_{k+1} \leftarrow \mathit{UB}_k$
		\EndIf
		\State $k \leftarrow k+1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}
As shown in \parencite[Chap. 8.9]{shetty}, however, the stopping criterion $g_k = 0$ may never be realized because the algorithm selects the subgradient $g_k$ arbitrarily. Usually, a stopping criterion is imposing a limit on the number of iterations performed by the algorithm. If we know the optimal value, which in general is unknown, we can impose the algorithm to stop when we reach a desired accuracy $\mathit{UB}_k < f^* + \epsilon$.

\subsubsection{Convergence}
We start with the assumption that \textit{SMs} are feasible only for those problems that do not require a high accuracy, as shown in \cite{subgrad_fra}. In fact, \textit{SM} requires $\Theta (1/\epsilon^2)$ iterations to attain an absolute error up to $\epsilon$. We can also note that the complexity does not depend on the size of the problem.

To study converge we first look for a bound on the distance to the optimal set, assuming that there exist an optimal solution, by \parencite[Theorem 7.4]{nonlinearrus} we have:
\begin{thm}
\label{thm:dmn}
Let the subgradient method use non-negative step sizes $\{\alpha_k\}$ such that
\begin{align}
\label{eq:dimin}
    &\sum_{k=1}^\infty\alpha_k = \infty\, \, \, and,\, \, \, \sum_{k=1}^\infty\alpha_k^2 < \infty.
\end{align}
Then the sequence $\{x_k\}$ generated by the subgradient method is convergent to a solution of the problem.
\end{thm}
\begin{proof}
By assuming $x^*$ is an optimal solution and considering that $f(x_k) - f(x^*) \geq 0$, then:
\begin{align}
    \begin{split}
        \norm{x_{k+1} - x^*}^2 & = \norm{x_k - \alpha_kg_k - x^*}^2\\
        & = \norm{x_k - x^*}^2 - 2\alpha_k\langle g_k\,,x_k - x^*\rangle + \alpha_k^2\norm{g_k}^2 \\
        & \leq \norm{x_k - x^*}^2 - 2\alpha_k(f(x_k) - f(x^*)) + \alpha_k^2\norm{g_k}^2\\
        & \leq \norm{x_k - x^*}^2 + \alpha_k^2\norm{g_k}^2,
        \label{proof:subsecond}
    \end{split}
\end{align}
where the third equation of (\ref{proof:subsecond}) comes from $g_k^T(x_k - x^*) \geq f(x_k) - f(x^*)$, that follows from the definition of subgradient.
By induction on $k$ we have:
\begin{align}
    \label{eq:bound}
    \norm{x_k - x^*}^2 &\leq \norm{x_0 - x^*}^2 - 2\sum_{l=0}^{k-1}\alpha_l(f(x_l)-f^*) + \sum_{l=0}^{k-1}\alpha_l^2\norm{g_l}^2\\
    & \leq \norm{x_0 - x^*}^2 + \sum_{l=0}^{k-1}\alpha_l^2\norm{g_l}^2\\
    & \leq \norm{x_0 - x^*}^2 + \sum_{l=0}^\infty\alpha_l^2\norm{g_l}^2
    \label{eq:last}
\end{align}
By \textbf{(\ref{eq:dimin})}, the sum in \textbf{(\ref{eq:last})} is bounded, thus the sequence $\{x_k\}$ is bounded. Using the result from \parencite[Theorem 7.2]{nonlinearrus} with \textit{learning rate} $\Bar{\tau}=0$, there exists an infinite set of iterations $\mathcal{K}$ such that for $k\in\mathcal{K}$, as $k\to\infty$, we have $f(x_k)\to f(x^*)$. We can choose an infinite set $\mathcal{K}_1\subset\mathcal{K}$ such that the subsequence $\{x_k\}$, with $k\in\mathcal{K}_1$, is convergent to $\hat{x}$ which must be an optimal solution and can be substituted to $x^*$. Choosing $l\in\mathcal{K}_1$, adding inequalities (\ref{proof:subsecond}) from $k=l$ to $m$ we obtain:
\begin{align*}
    \norm{x_{m+1}-\hat{x}}^2 \leq \norm{x_l - \hat{x}}^2 + \sum_{k=l}^\infty\alpha_k^2\norm{g_k}^2\, \, \, \, m=l+1,\, l+2,\, \dots.
\end{align*}
For each $\epsilon>0$ we can choose $l\in\mathit{K_1}$ such that $\norm{x_l - \hat{x}}^2 \leq \epsilon$, and $\sum_{k=l}^\infty\alpha_k^2\norm{g_k}^2\leq\epsilon$. Then $\norm{x_{m+1} - \hat{x}}^2\leq 2\epsilon$ for all $m\geq l$, which proves that the entire sequence $\{x_k\}$ is convergent to $\hat{x}$.
\end{proof}


To study convergence rate we refer to \parencite[Theorem 3.1]{notes_subgrad}:
\begin{thm}
\label{thm:subconv}
When $f:\mathbb{R}^n\to\mathbb{R}$ is convex and its subgradients are bounded by $L$, for any $g \in \partial f(x)$ at any $x$, subgradient descent starting at $x_0$ s.t. $\norm{x_0 - x^*} \leq R$ with step size $\alpha_l \leftarrow \frac{R}{\sqrt{k}\norm{g_l}}$ satisfy $f^*_k - f^* \leq \frac{LR}{\sqrt{k}}$ after k iterations.
\end{thm}
\begin{proof}
Rearranging (\ref{eq:bound}) we have:
\begin{align}
\begin{split}
\label{eq:subconv}
    2\sum_{l=0}^{k-1}\alpha_l(f(x_l) - f^*) &\leq \norm{x_0 - x^*}^2 - \norm{x_k - x^*}^2 + \sum_{l=0}^{k-1}\alpha_l^2\norm{g_l}^2\\
    & \leq \norm{x_0 - x^*}^2 + \sum_{l=1}^{k-1}\alpha_l^2\norm{g_l}^2
\end{split}
\end{align}
Let $\beta_l=\alpha_l\norm{g_l}$, $f_k^* = \min_{l<k}f(x_l)$. By the result obtained in (\ref{eq:subconv}) at the $k$-th iteration, we have:
\begin{align*}
    2(f_k^* - f^*)\sum_{l=0}^{k-1}\frac{\beta_l}{\norm{g_l}} \leq \norm{x_0 - x^*}^2 + \sum_{l=0}^{k-1}\beta_l^2 \leq \mathit{R}^2 + \sum_{l=0}^{k-1}\beta_l^2.
\end{align*}
Since the subgradients are bounded by $L$:
\begin{align*}
    \frac{2}{L}(f_k^* - f^*)\sum_{l=0}^{k-1}\beta_l \leq \mathit{R}^2+\sum_{l=0}^{k-1}\beta_l^2
\end{align*}

By rearranging, we obtain
\begin{align*}
    f_k^* - f^* \leq \frac{R^2+\sum_{l=1}^{k}\beta_l^2}{\frac{2}{L}\sum_{l=1}^k\beta_l}
\end{align*}
Since the bound is symmetric in $\{\beta_l\}$, the bound is minimized when all the $\beta_l$s are equal. For a given $k$, the bound is minimized at $\frac{R}{\sqrt{k}}$. The optimized bound is $f_k^* - f^* \leq \frac{LR}{\sqrt{k}}$.
\end{proof}
Thus, \textit{subgradient method} attains $\mathcal{O}(\frac{1}{\sqrt{k}})$-suboptimality after $k$ iterations, that means it obtains an {$\epsilon$-suboptimal} point after at most $\mathcal{O}(\frac{1}{\epsilon^2})$ iterations.

However, as seen in $\S$\ref{subsub:lfprop}, our loss function is not convex, so we can't state anything at this point about convergence of our setting, given that it doesn't respect the assumptions made for \textbf{Theorem \ref{thm:dmn}} and \textbf{Theorem \ref{thm:subconv}}. We postpone the discussion about practical convergence of the algorithm in the testing section.

