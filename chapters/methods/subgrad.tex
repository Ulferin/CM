\subsection{Subgradient Method}
The \textit{subgradient method} (\textit{SM}) is a minimization method used to minimize non-differentiable convex objective functions. It is not a descent method, in fact, chosen directions can lead to a temporary increase of the values generated by the iterative algorithm. To apply this method to our problem we need to adapt the objective function in such a way to make it convex and with a \textit{Lipschitz continuous} derivative.
\todo[inline]{controllare questa affermazione. Da chi non è definito un descent method? Detto da Ruszczyński in Nonlinear Optimization}
We can start by defining the subgradient method as:

Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$ a convex function not necessarily smooth, to minimize $f$ the \textit{subgradient method} uses the iteration
\begin{align*}
    \mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k {g}_k
\end{align*}
where $g_k \in \partial f(x_k)$ is any subgradient of $f$ at $x_k$, $\alpha_k > 0$ is the \textit{k}th stepsize and $x_k$ is the iterate at the step $k$, as seen in \parencite[Chap. 1]{subgrad_boyd} and \cite{notes_subgrad}. 

\begin{definition}
asd
\end{definition}

Following the definition of subgradients, a subgradient of $f$ at $x$ is any vector $g$ that satisfies the inequality ${f(y)\geq f(x) + g^T (y-x)}$ for all $y$. Following this definition, if the function $f$ is differentiable, the only choice we have for $g_k$ is $\nabla f(x_k)$, and the subgradient method reduces to the gradient method.
\todo[inline]{controllare questo fatto, inserire reference che mi permette di dire che questa cosa sia vera}

It is useful to keep track of the best point found during iterations, given the non-descent nature of the algorithm. We define:
\begin{align*}
    f_{best}^{(k)} = min\{f_{best}^{(k-1)}, f(x_k)\}
\end{align*}
We notice that since $f_{best}^{(k)}$ is decreasing at each iteration, it has a limit.

We can finally show a pseudocode for this kind of algorthms as following:
\begin{algorithm}[H]
    \caption{Basic subgradient method. Assuming starting point $x_1$ and subgradient at each point are given.}
	\label{alg:subalg}
	\begin{algorithmic}[1]
	    \State Initialize $\mathit{x_1} \in X$
	    \State Starting upper bound: $\mathit{UB}_1 \leftarrow f(x_1)$
	    \State Starting optimal value: $x^* \leftarrow x_1$
	    \State $k \leftarrow 1$
		\While{$termination\ conditions\ not\ met$}
		\State Find a subgradient of $f$ in $x_k$: $\xi_k \in \partial f(x_k)$
		\If {$\xi_k = 0$}
		\State Terminate with $x_k = x^*$
		\EndIf
		\State Select a direction: $d_k \leftarrow -\xi_k/\norm{\xi_k}$
		\State Select a step size: $\lambda_k > 0$
		\State $x_{k+1} \leftarrow x_k + \lambda_k d_k$
        \If{$f(x_{k+1}) < \mathit{UB}_k$}
            \State $\mathit{UB}_{k+1} \leftarrow f(x_{k+1})$
            \State {$x^* \leftarrow x_{k+1}$}
        \Else
            \State $\mathit{UB}_{k+1} \leftarrow \mathit{UB}_k$
		\EndIf
		\State $k \leftarrow k+1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}
However, the stopping criterion $\xi_k = 0$ may never be realized because the algorithm selects the subgradient $\xi_k$ arbitrarily. Usually, a stopping criterion is imposing a limit on the number of iterations performed by the algorithm. Another stopping condition to be used can be $x_{k+1} = x_k$ for any iteration. Moreover, if we know the optimal value, which in general is unknown, we can impose the algorithm to stop when we reach a desired accuracy $\mathit{UB}_k < f^* + \epsilon$.

\todo[inline]{controllare il fatto delle projection. In teoria se $X$ non è bounded ma è tutto $\mathbb{R}$ possiamo considerare $x_k$ come sempre appartenente al dominio.}

% \subsubsection{Step size rules}
% \todo[inline]{magari questo paragrafo si può eliminare del tutto. Non c'è bisogno di specificare i vari step size se poi non si usano. Meglio rimanere sul semplice e definire solo quello che si usa.}
% We can define different step size rules, each of them will have different results for what concerns convergence of the algorithm. In this section we give a general description of the most used rules and in the next section we give the coonvergence results for each of them.

% The different rules are:
% \begin{itemize}
%     \item \textit{Constant step size}: $\alpha_k = k$ is a constant, independent of $k$;
%     \item \textit{Constant step length}: $\alpha_k = \frac{h}{\norm{g_k}}$, where $h = \norm{x_{k+1} - x_k}$;
%     \item \textit{Square summable}: $\alpha_k$ satisfies
%     \begin{align*}
%         & \sum_{k=1}^\infty \alpha_k^2 < \infty,\ \ \ \ \sum_{k=1}^\infty \alpha_k = \infty
%     \end{align*}
%     \item \textit{Non-summable diminishing}: $\alpha_k$ satisfies:
%     \begin{align*}
%         \lim_{k\to\infty}\alpha_k = 0, \ \ \ \sum_{k=1}^\infty\alpha_k = \infty
%     \end{align*}
% \end{itemize}

\subsubsection{Convergence}
We start with the assumption that \textit{SMs} are feasible only for those problems that do not require a high accuracy, as shown in \cite{subgrad_fra}. In fact, \textit{SM} requires $\Theta (1/\epsilon^2)$ iterations to attain an absolute error up to $\epsilon$. We can also note that the complexity does not depend on the size of the problem.
% As shown in \parencite[Chap. 1.2]{subgrad_boyd} for constant step size and constant step length, the subgradient method is guaranteed to converge to some range from the optimal value $f^*$. Instead, for the diminishing step size and the square summable one, we have that the algorithm is provable convergent to the optimal value. This also applies when the function is differentiable, provided that the parameter $h$ is small enough (as seen in $\S$\ref{conv_mom}).

To study converge we first look for a bound on the distance to the optimal set:
\begin{align}
    \begin{split}
        \norm{x_{k+1} - x^*}^2 & = \norm{x_k - \alpha_kg_k - x^*}^2\\
        & = \norm{x_k - x^*}^2 + 2\alpha_k\langle g_k\,,x^* - x_k\rangle + \alpha_k^2\norm{g_k}^2 \\
        & \leq \norm{x_k - x^*}^2 + 2\alpha_k(f(x^*) - f(x_k)) + \alpha_k^2\norm{g_k}^2,
        \label{proof:subsecond}
    \end{split}
\end{align}
where the third equation comes from $f(y) \geq f(x) + g^T(y-x)$ holds for every $f:\mathbb{R}^n\to\mathbb{R}$ convex function, $g$ subgradient, $x\in \mathbf{dom}f$ and any $y$.

By induction on $k$ we have:
\begin{align*}
    \norm{x_k - x^*}^2 \leq \norm{x_0 - x^*}^2 - 2\sum_{l=0}^{k-1}\alpha_l(f(x_l)-f^*) + \sum_{l=0}^{k-1}\alpha_l^2\norm{g_l}^2
\end{align*}
Rearranging we have:
\begin{align}
\begin{split}
\label{eq:subconv}
    2\sum_{l=0}^{k-1}\alpha_l(f(x_l) - f^*) &\leq \norm{x_0 - x^*}^2 - \norm{x_k - x^*}^2 + \sum_{l=0}^{k-1}\alpha_l^2\norm{g_l}^2\\
    & \leq \norm{x_0 - x^*}^2 + \sum_{l=1}^{k-1}\alpha_l^2\norm{g_l}^2
\end{split}
\end{align}

At this point, to find convergence rate, we just have to specify which is the step size and solve the equation (\ref{eq:subconv}). Defining $f_k^* = \min_{l<k}f(x_l)$, for diminishing step size $\alpha_k \to 0$, if the subgradients are bounded by $L$, we obtain:
\begin{align*}
    f_k^* - f^* \leq \frac{\norm{x_0 - x^*}^2 + L^2\sum_{l=0}^{k-1}\alpha_l^2}{2\sum_{l=0}^{k-1}\alpha_l}
\end{align*}

Knowing that when $\alpha_k \to 0$ and $\sum_k \alpha_k = \infty$ we have $\frac{\sum_l\alpha_l^2}{\sum_l\alpha_l} \to 0$, \textit{SM} for diminishing step size converges. In the following we study which are the optimal step sizes. One of the crucial parameters to this choice is $\norm{x_0 - x^*}$.

As shown in \parencite[Theorem 3.1]{notes_subgrad}, we have:
\begin{thm}
When $f:\mathbb{R}^n\to\mathbb{R}$ is convex and its subgradients are bounded by $L$, for any $g \in \partial f(x)$ at any $x$, subgradient descent starting at $x_0$ s.t. $\norm{x_0 - x^*} \leq R$ with step size $\alpha_l \leftarrow \frac{R}{\sqrt{k}\norm{g_l}}$ satisfy $f^*_k - f^* \leq \frac{LR}{\sqrt{k}}$ after k iterations.
\end{thm}

\todo[inline]{la cosa interessante qua è che preso un qualsiasi fixed step size, basta che sia diminishing, si ha convergenza e il rate di convergence è quello mostrato da questo teorema. Trovare qualche reference che mi permetta di dire che questo è così. Controllare cosa cambia rispetto alle slide, in cui viene detto che il rate di convergence è quello anche per fixed step sizes senza conoscenza sulla funzione, quindi senza $x^*$ e $f^*$. Probabilmente anche nelle slide la stima del convergence rate è fatta conoscendo il valore ottimo della funzione, quindi probabilmente si può stimare solo conoscendolo. Molto probabilmente questo si può dire per studiare teoricamente il rate di convergenza, ma non si può applicare in pratica.}


% To study convergence we can refer to \parencite[Chap. 8.9, Theorem 8.9.2]{shetty}:
% \begin{thm}
% \label{thm:subconv}
% Let a problem $P$ defined as:
% \begin{align}
%     (P) = minimize \{f(x): x\in X\},
% \end{align}
% and assume that an optimum exists. Consider a subgradient optimization algorithm to solve the problem $P$, and suppose that the prescribed non-negative step size sequence $\{\lambda_k\}$ satisfies the condition $\{\lambda_k\}\to 0^+$ and $\sum_{k=0}^\infty \lambda_k = \infty$. Then, either the algorithm terminates finitely with an optimal solution, or else an infinite sequence is generated such that $\{\mathit{UB}_k\}\to f_*$ , where $f_*$ is the optimal solution of problem $P$.
% \end{thm}
% \begin{proof}
% Suppose that an infinite sequence $\{x_k\}$ is generated together with the sequence of \textit{upper bounds} $\{\mathit{UB_k}\}$. Since $\{\mathit{UB_k}\}$ is monotone nonincreasing, it has a limit point $\Bar{f}$. Consider any $\hat{x}$ s.t. for a given $\alpha > f(\hat{x}) > f^*$. Since $\hat{x} \in \mathis{S}_\alpha = \{x: f(x) \leq \alpha\}$, because $f$ is continuous, there exists $\rho > 0$ such that $\norm{x-\hat{x}} \leq \rho$ implies that $x \in \mathit{S}_\alpha$. We have that:
% Noting that, in equation (\ref{proof:subsecond}), the second term is negative and the third one is positive, given that $\alpha_k^2 \to 0$ faster than $\alpha_k$ does. This implies that $x_{k+1}$ is closer to $x^*$ than $x_k$.
% \todo[inline]{inserire proof qui, prenderla da libro oppure da slide. Quella delle slide sembrava un attimino più comprensibile.}
% \end{proof}

