\section{Artificial Neural Network}
The notation used to refer The implemented \textit{ANN} can be seen as a \textit{fully connected multilayer Perceptron} as referred in the literature and as shown in \cite{MLmitchell}. An \textit{ANN} is composed by an interconnection of units that can represented as the composition of two functions that will determine the real-valued output of \todo[inline]{magari descrivere che cosa è un perceptron}the unit. The two functions will be referred as the \textit{network function} and the \textit{activation function}, where the former computes the scalar product of the input vector with the weight vector of the current unit, the latter is the function that will directly determine the output of the current unit. As we will see, the choice of the activation function is particularly important and is preferred to be a nonlinear combination of its inputs, maintaining the property of being differentiable.

The implemented \textit{ANN} will be structured with multiple layers, each layer will have all the units fully connected with the adjacent layers and, as convention, we will refer to the first layer as \textit{input layer} and to the last layer as \textit{output layer}. The others will be referred as \textit{hidden layers}. Another important aspect when implementing an \textit{ANN} is the choice of the number of units. Later in this report will be shown how the exact number of units in each layer will be chosen, but we can already describe the structure of the input and the output layer. The former will contain a number of units that is the same as the number of features contained in the data that will be fed up to the \textit{ANN}, instead the latter will contain one binary output units for classification tasks and one real-output unit for regression tasks.\newline

In the following sections we will describe in more details which are the main aspects of the implemented \textit{ANN} like the network structure, the functions used to compute the output of each unit and the algorithm used to let the network learn the task at hand.

\subsection{Network structure}
As already pointed out in the previous section, the network structure will be composed of multiple layers. The number of layers and consequently, the number of units per layer will be determined by an empirical approach that will target the minimization of the empirical error on the tested data. This will be explained in more detail in the testing section.\newline

The input and the output layer will be of fixed dimensions, as the number of units for the former will be determined by the size of the input data and the dimension of the latter will be determined by the task to be completed. As we will see, the number of units in the output layer will be always of one, what will change instead is the type of the units, because for classification tasks we will use a binary unit, instead for a regression task we will use a real-output unit.

\subsection{Activation function}
\todo[inline]{Qui bisogna usare piecewise-linear activation function. Questo vuol dire che la funzione usata per determinare l'output di ogni unit all'interno della NN non è lineare e non è differenziabile. Ad esempio ReLu. Si possono illustrare i vantaggi nell'utilizzo di questo tipo di funzioni (come ad esempio non si ha fenomeno di vanishing gradient) e gli svantaggi (non si può calcolare derivata e quindi vanno usati metodi specifici) come ad esempio subgradient methods.}
\todo[inline]{In pratica per questo tipo di activation function si "aggira" il problema assegnando un valore arbitrario alla derivata nell'unico punto di non derivabilità della funzione. Ad esempio si può dire che $f'(x) = 0\ \forall x<0\ \land f'(x) = 1\ elsewhere $. Va spiegato poi come "reagisce" il momentum approach a questa scelta di calcolare la derivata.}
\todo[inline]{Usare una piecewise-linear activation function in teoria dovrebbe causare problemi allo standard descent con momentum, dato che in quel caso si minimizza una funzione derivabile. Magari questo è la parte in cui dobbiamo adattare il metodo al problema assegnato. (i.e. usare momentum descent su funzione non derivabile)}
The choice of the activation function is a crucial step for the construction of the \textit{ANN}. This function will directly determine what is the output of each of the units in the network, depending on the result of the scalar product of the unit input and the unit weights vectors. The activation function, to be useful, needs to have determined properties, like differentiability, \textbf{AGGIUNGERE ALTRE PROPRIETà CHE LA RETE DEVE AVERE}.\newline

The mainly used ones, which will be used also for the purposes of this project, are:
\begin{itemize}
    \item Sigmoid
    \item TanH
\end{itemize}

\subsection{Loss function}
\todo[inline]{In particolare parlare di MSE, perché viene usata questa metrica per stimare l'errore e quali proprietà ha questa funzione.}
The loss function is used to estimate the error at the output of the network given the vector of the expected values for the data that are fed up into the network. The loss function is also the function to be minimized as the main aim for the learning algorithm. The error computed via the loss function is usually used as a termination condition for the learning algorithm. This is done via various algorithm, and in particular for our case via subgradient methods and standard momentum descent approach.\newline

In our case we decided to use as a function to measure the error in the prediction the \textit{MSE} that represents the sum, over all the available data, of the squared differences between the predicted value and the actual one.\newline

\subsubsection{Derivation of Loss function}
As a preliminary step for the backpropagation algorithm, we illustrate here how the gradient of the loss function is computed. The result is then used to update the value of the weight vector for each unit. In the following the derivation of the loss function.\newline\newline
\textbf{QUI INSERIRE DERIVAZIONE LOSS FUNCTION}

\subsection{Backpropagation algorithm}
\todo[inline]{Attenzione perché backprop è riferito in \cite{bengio} come l'algoritmo utilizzato per calcolare il gradiente e non i pesi. L'algoritmo che cambia i pesi è ad esempio SGD.}
The backpropagation algorithm \parencite[see][chapter 4]{haykin_neural_2009} is used to learn weight vectors for a multilayer network, given the network inputs and the fixed network structure (i.e. units and interconnections). This algorithm employs a gradient descent approach to attempt to minimize the squared error between the network output values $\textbf{y}$ and the target values $\hat{\textbf{y}}$ associated to these outputs. This algorithm is also described in \cite{MLmitchell} and is composed by two main parts:
\begin{itemize}
    \item \textbf{Forward phase}: data traverse the network from the input units to the output units, in such a way the network's output value is generated and used in the next phase to be compared with the expected output to estimate the error.
    \item \textbf{Backward phase}: the error is computed by comparing the network's output with the expected one. The computed error at each layer is then propagated back in the previous layer. At each backward step an update rule is used to modify the weight vectors of each layer in order to have smaller error in the next iteration.
\end{itemize}
The main aim for the backpropagation algorithm is to minimize the error function via automatic fine-tuning of the weight vector. Each layer will proceed to update its weight vector depending on the layer's contribute on the total error. This minimization can be achieved in different ways utilizing the gradient of the loss function computed with the results obtained with the forward phase, adding a momentum.\newline
\todo[inline]{qui si potrebbe spiegare la differenza tra batch e online learning, dove il secondo è consigliato (in Haykin) per grandi problemi di classificazione etc, i quali richiederebbero grandi risorse in termini di spazio. Nel nostro caso, con il MONK dataset questo non è il caso, quindi si è preferito usare batch.}

