\section{Artificial Neural Network}
The implemented \textit{ANN} can be seen as a \textit{fully connected multilayer Perceptron}. An \textit{ANN} is composed by an interconnection of units, each one of them can be represented as the composition of two functions that will determine the its real-valued output given the fixed weight vector and the input from the previous layer. The two functions will be referred as the \textit{network function} and the \textit{activation function}, where the former computes the scalar product of the input vector with the weight vector of the current unit, the latter is the function that will directly determine the output of the current unit. In our particular case the activation function is required to be a \textit{piecewise-linear activation function}.

The implemented \textit{ANN} will be structured with multiple layers, each layer will have all the units fully connected with the adjacent layers and, as convention, we will refer to the first layer as \textit{input layer} and to the last layer as \textit{output layer}. The others will be referred as \textit{hidden layers}. Another important aspect when implementing an \textit{ANN} is the choice of the number of units. Later in this report will be shown how the exact number of units in each layer will be chosen, but we can already describe the structure of the input and the output layer. The former will contain a number of units that is the same as the number of features contained in the data that will be fed up to the \textit{ANN}, instead the latter will contain one binary output units for classification tasks and one real-output unit for regression tasks.\newline

To simplify the development of the \textit{ANN} we plan to fix the number of hidden layer and the number of units per layer, changing only the input/output layers depending on the task to be performed. The process involved in the determination of this fixed characteristics of the \textit{ANN} will be described in the testing section.\newline
In the following sections we will describe in more details which are the main aspects of the implemented \textit{ANN} like the network structure, the functions used to compute the output of each unit and the algorithm used to let the network learn the task at hand.

\subsection{Activation function}
The choice of the activation function is a crucial step for the construction of the \textit{ANN}. This function will directly determine what is the output of each of the units in the network, depending on the result of the scalar product of the received input vector and the unit weights vectors.\newline
The activation function, for this project, is required to be a \textit{pieceweise-linear function}, so the choice can be restricted between the two most popular among them:
\begin{itemize}
    \item \textbf{ReLU}:
        \begin{itemize}
            \item defined as: $f(x) = max(0,x)$;
            \item we can impose the derivative to be:
            $f'(x) = \begin{cases} 
                0 & x<0 \\
                1 & x\geq 0 
                \end{cases}$
        \end{itemize}
    \item \textbf{Leaky ReLU}:
        \begin{itemize}
            \item defined as:
            $f(x) = \begin{cases}
                \alpha x & x\leq 0 \\
                x & x>0
            \end{cases}$
            \item also in this case we can impose the derivative to be:
            $f'(x) = \begin{cases}
                \alpha & x\leq0 \\
                1 & x>0
            \end{cases}$
        \end{itemize}
        Where $\alpha$ determines the slope of the negative part of the function, and usually is chosen like $\alpha=0.01$.\newline
\end{itemize}
The main characteristics of these functions are:
\begin{itemize}
    \item Sparse activation;
    \item Helps avoiding vanishing gradient;
    \item Efficient computation;
\end{itemize}
These functions are widely used due to the fact that they can help avoiding the problem of \textit{vanishing gradients} when using the backpropagation algorithm described in \S\ref{backprop}. Our main choice is related to the use of the ReLU function due to its simplicity. The only problem is that it is not entirely differentiable, in particular there is only one point (i.e. $x=0$) where the derivative is not defined. As shown in \todo[inline]{trovare qualche paper che spiega come ReLU con derivata "fixata" vada bene comunque} the slight modification of the derivative with $f'(0) = 1$ leads to good results and do not improve the convergence of the learning algorithm.

\subsection{Backpropagation algorithm}
\label{backprop}
The backpropagation algorithm \parencite[see][Chap. 6.5]{bengio}, in a multi-layer neural network, is used to compute the gradient of the cost function and it will be used by the learning algorithm to minimize the squared error between the network output values $\hat{\textbf{y}}$ and the target values $\textbf{y}$ associated to these outputs. The backpropagation algorithm can then be used to efficiently compute the derivative of the composition of functions resulting from the \textit{ANN} computation.\\newline
This algorithm is also described in \cite{MLmitchell} and is composed by two main parts:
\begin{itemize}
    \item \textbf{Forward phase}: data traverse the network from the input units to the output units, in such a way the network's output value is generated and used in the next phase to be compared with the expected output to estimate the error. The procedure is shown in \hyperref[alg:fp]{\textbf{Algorithm \ref{alg:fp}}}.
    \item \textbf{Backward phase}: the error is computed by comparing the network's output with the expected one. The computed error at each layer is then propagated back in the previous layer. At each backward step the \textit{Chain Rule of Calculus} is used to compute the partial derivative of the unit function related to the current layer's weights. This phase is defined in \hyperref[alg:bp]{\textbf{Algorithm \ref{alg:bp}}}.
\end{itemize}

\begin{algorithm}[H]
	\caption{Forward propagation}
	\label{alg:fp}
	\begin{algorithmic}[1]
		\Procedure{Forward propagation}{}
		\State $\mathbf{\hat{y}}_{0} = \mathbf{x}$
		\For{$k = 1, \ldots, l$}
		\State $\mathbf{a}_{k} = \mathbf{b}_{k} + \mathbf{W}_{k}\mathbf{\hat{y}}_{k - 1}$
		\State $\mathbf{\hat{y}}_{k} = f(\mathbf{a}_{k})$
		\EndFor
		\State $\mathbf{\hat{y}} = \mathbf{\hat{y}}_{l}$
		\State $J = L(\mathbf{\hat{y}}, \mathbf{y}) + \lambda \Omega(\theta)$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\hyperref[alg:fp]{\textbf{Algorithm \ref{alg:fp}}} proceeds to compute the composition of functions that represents the network until the output unit are reached where the network's output value ${\hat{\textbf{y}}}$ is produced. Each of the ${\hat{\textbf{y}}}_i$ represents the output vector coming from the layer $\textit{i}$. Once the predicted output is produced, the algorithm proceeds into computing the loss function $L(\mathbf{\hat{\textbf{y}}}, \mathbf{\textbf{y}})$ that will estimate the error for the given output vector and, by adding this value to a regularizer $\Omega(\theta)$ we obtain the total cost $\textit{J}$.\newline

The gradient of the cost will then be computed by the next phase and passed to the optimizer. 

\begin{algorithm}[H]
	\caption{Backward computation}
	\label{alg:bp}
	\begin{algorithmic}[1]
		\Procedure{Backward propagation}{}
		\State $\mathbf{g} \leftarrow \nabla_{\hat{\mathbf{y}}}J = \nabla_{\hat{\mathbf{y}}}
		L(\mathbf{\hat{y}}, \mathbf{y})$
		\For{$k = l, l - 1, \ldots, 1$}
		\State $\mathbf{g} \leftarrow \nabla_{\mathbf{a}_{k}}J = \mathbf{g} \ \odot \
		f'(\mathbf{a}_{k})$
		\State $\nabla_{\mathbf{b}_{k}}J = \mathbf{g} \ + \ \lambda \nabla_{\mathbf{b}_{k}}
		\Omega(\theta)$
		\State $\nabla_{\mathbf{W}_{k}}J = \mathbf{g}\mathbf{\hat{y}}_{k - 1}^{T} \ + \ \lambda
		\nabla_{\mathbf{W}_{k}} \Omega(\theta)$
		\State $\mathbf{g} = \nabla_{\mathbf{\hat{y}}_{k - 1}}J = \mathbf{W}_{k}^{T}\mathbf{g}$
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\todo[inline]{Questo non è compito principale della backpropagation, ma è il compito principale dell'algoritmo di ottimizzazione, come ad esempio momentum etc.}
The main aim for the backpropagation algorithm is to minimize the error function via automatic fine-tuning of the weight vector. Each layer will proceed to update its weight vector depending on the layer's contribute on the total error. This minimization can be achieved in different ways utilizing the gradient of the loss function computed with the results obtained with the forward phase, adding a momentum.\newline

\subsection{Loss function}
\todo[inline]{In particolare parlare di MSE, perché viene usata questa metrica per stimare l'errore e quali proprietà ha questa funzione.}
The loss function is used to estimate the error at the output of the network given the vector of the expected values for the data that are fed up into the network. The loss function is also the function to be minimized as the main aim for the learning algorithm. The error computed via the loss function is usually used as a termination condition for the learning algorithm. This is done via various algorithm, and in particular for our case via subgradient methods and standard momentum descent approach.\newline

In our case we decided to use as a function to measure the error in the prediction the \textit{MSE} that represents the sum, over all the available data, of the squared differences between the predicted value and the actual one.\newline